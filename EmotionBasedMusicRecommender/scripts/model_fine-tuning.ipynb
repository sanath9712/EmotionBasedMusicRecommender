{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMnRH8XGO787ziqwdZwqjFG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_p7PQA6dO5NC"},"outputs":[],"source":["\n","#Create the dataset\n","dataset = EmotionDataset(input_ids, attention_masks, labels)\n","\n","#DataLoader\n","train_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n","\n","model = BertForSequenceClassification.from_pretrained(\n","    'bert-base-uncased',\n","    num_labels=len(emotions)\n",")\n","\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","\n","loss_fn = BCEWithLogitsLoss()\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","epochs = 3\n","for epoch in range(epochs):\n","    model.train()\n","    total_loss = 0\n","    print(f\"Starting epoch {epoch+1}/{epochs}\")\n","\n","\n","    batch_progress = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Batch\")\n","    for step, batch in batch_progress:\n","        b_input_ids = batch['input_ids'].to(device)\n","        b_input_mask = batch['attention_mask'].to(device)\n","        b_labels = batch['labels'].to(device)\n","\n","        model.zero_grad()\n","\n","        outputs = model(b_input_ids, attention_mask=b_input_mask)\n","\n","        loss = loss_fn(outputs.logits, b_labels)\n","        total_loss += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        batch_progress.set_postfix({'Training Loss': '{:.3f}'.format(loss.item())})\n","\n","    avg_train_loss = total_loss / len(train_loader)\n","    print(f\"Epoch {epoch+1}/{epochs} - Average training loss: {avg_train_loss}\")\n","\n","# Save the model\n","model.save_pretrained('./saved_model')\n","tokenizer.save_pretrained('./saved_model')"]}]}